---
title: 참새작 AI 만들기 1 - 환경
toc: true
date: 2022-03-19 17:05:52
tags: [SuzumeAI, AI, Reinforcement Learning]
categories: [AI, Reinforcement Learning]
---

참새작의 환경은 다음과 같이 구성된다.

## 1. 패
참새작의 패는 11종류, 4세트인 44장으로 구성된다. 44장을 모두 구분하기 위해서 각각의 패에 0~43 사이의 번호를 부여한다. 이때, 번호와 패의 관계는 두 방법으로 정의 될 수 있다.

1. [세트] [종류] : 숫자가 1인 패 = [0, 1, 2, 3]
2. [종류] [세트] : 숫자가 1인 패 = [0, 11, 22, 33]

패를 정렬할 때, 종류 순으로 정렬해야 하는 것이 보기에 좋으므로, 첫번째 방법으로 정의한다.

## 2. 상태 (State)
패는 다음 중 한 곳에 있다.

* 패산
* 도라표시패
* 각 플레이어의 손패
* 각 플레이어의 버림패

패를 고유한 번호로 인식할 때, 패는 두 곳에 동시에 존재할 수 없다. 그러므로 패산의 순서, 플레이어가 패를 버린 순서를 무시하면 현재 게임의 상태는 각 패의 위치를 저장하는 크기가 44인 배열로 표현할 수 있다.  
(패를 버린 순서는 마작에 있어서 손패를 예측하는 데 꽤 중요하지만 그러면 모델의 크기가 커진다.)

### 쯔모한 패와 버림패
쯔모한 패와 버림패는 특수한 패이다. (여기서 말하는 '버림패'는 갓 버린 패를 의미한다.) 이 패는 각 플레이어들이 화료 조건을 따지는 패이기 때문이다. 그러므로 이 패는 다른 손패, 버림패와 구분될 필요가 있다.

### **쯔모 턴**과 **론 턴**
참새작의 행동과정은 다음과 같이 이루어진다.

* 자신의 차례 : 패산에서 쯔모한다.
    * 화료 조건을 만족하고, 지금 화료하는 것이 이득이다 : 화료한다.(쯔모)
    * 그렇지 않다 : 손패에서 패 한장 버린다.
* 상대방의 차례 : 상대방이 패를 버렸다.
    * 화료 조건을 만족하고, 지금 화료하는 것이 이득이다 : 화료한다.(론)
    * 그렇지 않다 : 지나간다.

위에서 확인 가능한 것은, 참새작의 차례는 두 부분으로 나뉘어진다는 것이다. 하나는 자기 차례인 플레이어가 패를 뽑고 버리거나 쯔모하는 과정, 나머지 하나는 자기 차례인 플레이어가 패를 버리고 다른 플레이어가 론을 부르는 과정이다. 전자를 **쯔모 턴**, 후자를 **론 턴**이라고 하자.

## 3. 행동 (Action)
한 차례가 두 부분으로 나뉜다는 것은 행동에도 두 가지 종류가 있다는 것을 의미한다. **쯔모 턴**에서 할 수 있는 행동(패를 버림, 쯔모)과 **론 턴**에서 할 수 있는 행동(패스, 론)은 서로 구분된다. 차례인 플레이어는 론을 할 수 없고, 차례가 아닌 플레이어는 패를 버릴 수 없다. 그러나 각 행동을 구분할 수 있다면 한 변수에 넣어도 된다. 각 행동에 대하여 서로 다른 번호를 부여하는 것이다. 패스와 론을 각각 0과 1로 부여하면 보기에는 좋으나 패를 버리는 행동과 햇갈릴 수 있으므로, 각각의 행동을 서로 구분되는 번호로 매기는 것이 더 안정적이다.

또한 패를 버리는 행위에 번호를 매기는 방법도 두 종류가 있다.

1. 손패의 위치(0~5 사이)
2. 버리는 패의 번호(0~43 사이)

첫번째 방법으로 하면 Action Space의 크기가 줄어드는 반면, 학습하기에는 힘들어진다. 예를 들어 내가 손패에서 다섯번째 패를 버렸다고 하자. 그 패가 *적5*인지 *발*인지 어떻게 확신할 것인가? 그래서 Action Space가 커지더라도 학습은 편한 두번째 방법을 이용한다. 

'패산에서 쯔모'는 매 턴마다 그 어떤 행동보다 먼저, 그리고 반드시 발생하는 행동이다. 행동이지만 플레이어의 의지와는 관계없이 진행된다는 점에서 '환경'의 일부라고 볼 수 있다. 그러므로 이 행동은 번호를 매기지 않는다.

## 4. 보상 (Reward)
* 플레이어가 환경을 관측해서 행동을 하면, 환경은 현재 상태와 행동에 따른 보상를 플레이어에게 준다.
* 플레이어는 보상을 최대로 하는 방향으로 학습한다.

위의 사실로 인해 보상함수를 잘 설계하는 것 또한 중요하다. 참새작에서 사용할 수 있는 보상함수의 종류는 다음과 같다.

* 최종적인 순위
* 각 국에서의 점수의 증감
* 도라, 적패, 확정된 점수
* 완성된 몸통의 수 등

아래로 내려갈수록 보상을 얻는 데 더 오래걸리지만 더 확실한 보상을 얻는다. 모든 정책의 목표는 게임에서 1등하는 것이고, 점수의 증감 역시 순위에 직접적인 영향을 미친다. 그러나 한 게임은 플레이어 수 만큼의 국으로 이루어져 있고, 국 또한 최소 18번의 차례가 지나간다. 즉, 보상을 얻는데 꽤 오래 걸린다. 그에 대한 보완책으로 아래 두 방법을 같이 사용할 수 있다.

## 결론

* 환경은 플레이어(또는 에이전트)에게 상태를 제공하며, 플레이어는 환경에게 행동을 제공하여 보상을 얻는다.
* 모든 패에 고유한 번호를 붙였을 때, 각 패는 정해진 위치에 단 하나만 존재할 수 있다. 그러므로 상태를 각 패의 위치를 저장하는 배열로 표현할 수 있다.
* 참새작의 차례는 **쯔모 턴**과 **론 턴**으로 나뉜다. 그에 따라 행동도 두 종류로 구분이 된다. 그러나 각 행동에 고유한 번호를 붙이면, 한 변수로 표현이 가능하다.
* 보상함수의 설계는 우리의 목표에 중요한 영향을 끼친다. 참새작에서 확실한 보상(최종적인 순위, 각 국에서의 점수의 증감)은 얻는 데 시간이 걸린다. 그러므로 이를 보완할 보상함수를 같이 쓰는 것이 좋을 것 같다.
